{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c246e5a-fd05-4eb2-bd5c-d9bd2c94e6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "max_length = 128  # max sequence length for each document/sentence sample\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_name, do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e81adc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data_path = './ABSA/Steam/data_with_clusters.json'\n",
    "reviews = []\n",
    "with open(data_path, \"r\") as f:\n",
    "    for line in f:\n",
    "        reviews.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b8d4812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def load_index(index_dir):\n",
    "    assert os.path.exists(index_dir)\n",
    "    with open(os.path.join(index_dir, 'train.index'), 'r') as f:\n",
    "        train_index = [int(x) for x in f.readline().split(' ')]\n",
    "    with open(os.path.join(index_dir, 'validation.index'), 'r') as f:\n",
    "        valid_index = [int(x) for x in f.readline().split(' ')]\n",
    "    with open(os.path.join(index_dir, 'test.index'), 'r') as f:\n",
    "        test_index = [int(x) for x in f.readline().split(' ')]\n",
    "    return train_index, valid_index, test_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15267e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sentence(sentence_dir):\n",
    "    gen_train, gen_valid, gen_test = [], [], []\n",
    "    with open(os.path.join(sentence_dir, 'generated-train.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)//4):\n",
    "            gen_train.append(lines[i*4+2].strip())\n",
    "    with open(os.path.join(sentence_dir, 'generated-validation.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)//4):\n",
    "            gen_valid.append(lines[i*4+2].strip())\n",
    "    with open(os.path.join(sentence_dir, 'generated-test.txt'), 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for i in range(len(lines)//4):\n",
    "            gen_test.append(lines[i*4+2].strip())\n",
    "    return gen_train, gen_valid, gen_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e237355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index, valid_index, test_index = load_index(\"/remote-home/jianghaitian/stage1/Steam/1/\")\n",
    "gen_train, gen_valid, gen_test = load_sentence(\"save/Steam/\")\n",
    "gen_train_lab = [float(reviews[i]['rating']) for i in train_index]\n",
    "gen_valid_lab = [float(reviews[i]['rating']) for i in valid_index]\n",
    "gen_test_lab = [float(reviews[i]['rating']) for i in test_index]\n",
    "\n",
    "gen_train_enc = tokenizer(gen_train, truncation=True, padding=True, max_length=max_length)\n",
    "gen_valid_enc = tokenizer(gen_valid, truncation=True, padding=True, max_length=max_length)\n",
    "gen_test_enc = tokenizer(gen_test, truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63f6f523-6965-45e5-b328-49da0a34f5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dataset\n",
    "random.shuffle(reviews)\n",
    "\n",
    "# len(reviews)=789032, 6.3:2:1.7\n",
    "train_texts = [' '.join(d['all_tokens']) for d in reviews[:500000]]\n",
    "train_labels = [float(d['rating']) for d in reviews[:500000]]\n",
    "\n",
    "valid_texts = [' '.join(d['all_tokens']) for d in reviews[500000:650000]]\n",
    "valid_labels = [float(d['rating']) for d in reviews[500000:650000]]\n",
    "\n",
    "test_texts = [' '.join(d['all_tokens']) for d in reviews[650000:]]\n",
    "test_labels = [float(d['rating']) for d in reviews[650000:]]\n",
    "\n",
    "train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=max_length)\n",
    "valid_encodings = tokenizer(valid_texts, truncation=True, padding=True, max_length=max_length)\n",
    "test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa746256-b717-46cf-9dba-50769f01a73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsGroupsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
    "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# convert our tokenized data into a torch Dataset\n",
    "train_dataset = NewsGroupsDataset(train_encodings, train_labels)\n",
    "valid_dataset = NewsGroupsDataset(valid_encodings, valid_labels)\n",
    "test_dataset = NewsGroupsDataset(test_encodings, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "227d92d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_train_dtst = NewsGroupsDataset(gen_train_enc, gen_train_lab)\n",
    "gen_valid_dtst = NewsGroupsDataset(gen_valid_enc, gen_valid_lab)\n",
    "gen_test_dtst = NewsGroupsDataset(gen_test_enc, gen_test_lab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6bb1598c-c314-4403-867d-b036b225aec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=1).to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff90b902-15c2-4466-a332-a0f84b7ae387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, mean_squared_error, mean_absolute_error  # accuracy is just micro F-1\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    # preds = pred.predictions.argmax(-1)\n",
    "    # calculate accuracy using sklearn's function\n",
    "    # acc = accuracy_score(labels, preds)\n",
    "    mse = mean_squared_error(labels, pred.predictions)\n",
    "    mae = mean_absolute_error(labels, pred.predictions)\n",
    "    return {'mse': mse, 'mae': mae}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "122f8b6e-ffcc-4fa4-990a-58696f0bc7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=1,              # total number of training epochs\n",
    "    per_device_train_batch_size=128,  # batch size per device during training\n",
    "    per_device_eval_batch_size=128,   # batch size for evaluation\n",
    "    warmup_steps=10,                 # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    "    load_best_model_at_end=True,     # load the best model when finished training\n",
    "    logging_steps=128,                # log weights each `logging_steps`\n",
    "    eval_steps=128,                   # evaluate each `eval_steps`\n",
    "    save_steps=128,                   # save weights each `save_steps`\n",
    "    evaluation_strategy=\"steps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "07ad15ab-27e4-44e5-bcf2-1baef3cefd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=train_dataset,         # training dataset\n",
    "    eval_dataset=valid_dataset,          # evaluation dataset\n",
    "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1d5857f0-60a6-441e-93c6-a5408ec7fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 500000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 128\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 977\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='977' max='977' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [977/977 31:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Mse</th>\n",
       "      <th>Mae</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.823400</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.003662</td>\n",
       "      <td>0.060437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.011600</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.014648</td>\n",
       "      <td>0.120976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>384</td>\n",
       "      <td>0.008500</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.006818</td>\n",
       "      <td>0.082539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>512</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.009896</td>\n",
       "      <td>0.099453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>640</td>\n",
       "      <td>0.006500</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.010763</td>\n",
       "      <td>0.103722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>768</td>\n",
       "      <td>0.006100</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>0.010269</td>\n",
       "      <td>0.101316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>896</td>\n",
       "      <td>0.005800</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.011001</td>\n",
       "      <td>0.104870</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 150000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-128\n",
      "Configuration saved in ./results/checkpoint-128/config.json\n",
      "Model weights saved in ./results/checkpoint-128/pytorch_model.bin\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-256\n",
      "Configuration saved in ./results/checkpoint-256/config.json\n",
      "Model weights saved in ./results/checkpoint-256/pytorch_model.bin\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-384\n",
      "Configuration saved in ./results/checkpoint-384/config.json\n",
      "Model weights saved in ./results/checkpoint-384/pytorch_model.bin\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-512\n",
      "Configuration saved in ./results/checkpoint-512/config.json\n",
      "Model weights saved in ./results/checkpoint-512/pytorch_model.bin\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-640\n",
      "Configuration saved in ./results/checkpoint-640/config.json\n",
      "Model weights saved in ./results/checkpoint-640/pytorch_model.bin\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-768\n",
      "Configuration saved in ./results/checkpoint-768/config.json\n",
      "Model weights saved in ./results/checkpoint-768/pytorch_model.bin\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 150000\n",
      "  Batch size = 128\n",
      "Saving model checkpoint to ./results/checkpoint-896\n",
      "Configuration saved in ./results/checkpoint-896/config.json\n",
      "Model weights saved in ./results/checkpoint-896/pytorch_model.bin\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-128 (score: 0.0036616413854062557).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=977, training_loss=0.376339291659934, metrics={'train_runtime': 1899.1399, 'train_samples_per_second': 263.277, 'train_steps_per_second': 0.514, 'total_flos': 3.2888586624e+16, 'train_loss': 0.376339291659934, 'epoch': 1.0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4f28489b-a047-4595-a6dc-626554fb04a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 139032\n",
      "  Batch size = 128\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1769' max='272' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [272/272 21:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0036597298458218575,\n",
       " 'eval_mse': 0.003659711219370365,\n",
       " 'eval_mae': 0.06042076647281647,\n",
       " 'eval_runtime': 111.9866,\n",
       " 'eval_samples_per_second': 1241.506,\n",
       " 'eval_steps_per_second': 2.429,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cad2fbf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 500000\n",
      "  Batch size = 128\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.0036606364883482456,\n",
       " 'eval_mse': 0.0036606553476303816,\n",
       " 'eval_mae': 0.06042814627289772,\n",
       " 'eval_runtime': 429.9466,\n",
       " 'eval_samples_per_second': 1162.935,\n",
       " 'eval_steps_per_second': 2.272,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c30c82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 211692\n",
      "  Batch size = 128\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.004061573185026646,\n",
       " 'eval_mse': 0.004061552230268717,\n",
       " 'eval_mae': 0.06359144300222397,\n",
       " 'eval_runtime': 167.7996,\n",
       " 'eval_samples_per_second': 1261.576,\n",
       " 'eval_steps_per_second': 2.467,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(gen_train_dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b11ff98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26680\n",
      "  Batch size = 128\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.004069428890943527,\n",
       " 'eval_mse': 0.004070572555065155,\n",
       " 'eval_mae': 0.06367037445306778,\n",
       " 'eval_runtime': 17.8615,\n",
       " 'eval_samples_per_second': 1493.717,\n",
       " 'eval_steps_per_second': 2.967,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(gen_valid_dtst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac3b8216",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 26635\n",
      "  Batch size = 128\n",
      "/root/anaconda3/envs/torch/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.004060035105794668,\n",
       " 'eval_mse': 0.004065202549099922,\n",
       " 'eval_mae': 0.0636281669139862,\n",
       " 'eval_runtime': 17.8206,\n",
       " 'eval_samples_per_second': 1494.619,\n",
       " 'eval_steps_per_second': 2.974,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate(gen_test_dtst)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
